Programming Model（编程模型
这一计算获得并输入一个k/v键值对集合，然后生成并输出一个k/v键值对集合。
MapReduce库的用户通过Map和Reduce这两个函数来表达该计算逻辑。

# 基本的MapReduce编程模型

## MapReduce编程模型

1.Map函数是由用户编写的，其获得一个输入的k/v对并生成一个中间态的k/v对。
MapReduce库对所有的k/v对进行分组，使得所有有着相同中间态key值的k/v对的value值组合在一起，然后将它们传递给Reduce函数。

2。Reduce函数也是由用户编写的，其接收一个中间态的key值和与该键对应的一组value值的集合。
它会将这些value值进行统一的合并以形成一个可能更小的value值集合。
通常，每次reduce调用只会生成零个或一个输出值。这个中间态的value集合通过一个迭代器提供给用户的reduce函数。
这允许我们得以处理那些无法被完整放入内存的，过大的列表集合。
map(String key, String value):
    // key: document name
    // value: document contents
    for each word w in value:
        EmitIntermediate(w,"1");
reduce(String key, Iterator values):
    // key: a word
    // values: a list of counts
    int result = 0;
    for each v in values:
        result += ParseInt(v);
    Emit(AsString(result));

这个map映射函数生成每一个单词以及其出现的次数（在这个简单的例子中次数恰好是1）。
reduce函数则累加每一个生成的特定单词其所有的出现计数。

## 类型
用户提供的map和reduce函数在类型上是有关联的：
map (k1,v1) --> list(k2,v2)
reduce (k2,list(v2)) --> list (v2)

## 更多例子
***分布式Grep***
map函数如果匹配某个给定规则则输出对应的那一行。
reduce函数是一个恒等函数，其只是将所输入的中间数据原封不动的复制到输出（恒等函数：f(x) = x, 即输入=输出）
***URL访问频率计数***
map函数处理网页请求的处理日志，并且输出<URL,1>的键值对。
reduce函数累加所有具有相同URL键值对的value值，并且输出一个<URL,总访问数>的键值对。
***反向web链接图***
map函数从每一个源页面（source）中找出每一个目标页URL（target）的链接，输出（target，source）格式的kv对。
reduce函数将所有具有相同target目标页的所有源页面（source）结合在一起组成一个列表，输出这样一个kv对（target,list(source)）
***每台主机的检索词向量***
汇总从一个或一系列文档中出现的最重要的单词作为检索词向量（term-vector），生成以<word(单词),frequency(出现频次)>格式的kv对列表。
map函数针对每一个输入的文档，输出一个<hostname(主机名),term vector(检索词向量)>的kv对（主机名是从文档的URL中提取出来的）。
reduce函数接收一个给定host下基于每个文档的所有term-vectors(检索词向量)。
将这些检索词向量进行累加，抛弃掉一些出现频率较低的检索词项然后返回最终的<hostname(主机名),term vector(检索词向量)>的kv对

***倒排索引***
map函数解析每一个文档，然后输出一连串<word(单词)，documentID(文档ID)>格式的kv对。
reduce函数接收一个给定单词对应的所有kv对，针对文档ID进行排序然后返回一个<word(单词),_list_documentID(文档ID列表)>格式的kv对。
所有输出的kv对集合构成了一个简单的倒排索引。基于此，我们能简单的增加记录每一个单词(在这些文档中)的位置的计算功能。

***分布式排序***
map函数提取每一个记录中的key值，然后返回一个<key,record>格式的kv对。
reduce函数对所有的kv对不做修改直接返回。
该计算依赖于后续的有序性机制

# 实现
通过交换式以太网互相连接起来的大型商品级PC集群
在我们的环境中:
(1) 机器通常是有着x86架构的双处理器的、运行linux操作系统的平台，每台机器有2-4GB的内存。
(2) 使用商品级的网络硬件 - 通常每台机器的带宽为100M/s或者1GB/s，但其平均(实际使用的)带宽远小于整个网络带宽的一半。
(3) 整个集群是由几百或几千台机器所组成的，因此机器故障是频繁出现的。
(4) 存储是由直接连接到独立机器上的IDE硬盘提供的。存储在这些磁盘上的数据由一个内部自研的分布式文件系统来管理。这一文件系统采用复制机制，旨在不可靠的硬件之上实现可用性和可靠性。
(5) 用户提交作业(job)给一个调度系统。每个作业都由一系列的任务(task)组成，任务被调度器映射到内部集群中的一组可用机器上去执行。
通过将输入的数据自动分割为M份，map调用得以分布在多个机器上调用执行。拆分后的输入数据可以被不同的机器并行的处理。
通过一个分区函数将中间态的key值空间划分为R份(例如: hash(key) mod R, 对key做hash后再对R求模)，Reduce调用也得以分布式的执行。
分区的个数(R)和分区函数都由用户来指定

内嵌于用户程序中的MapReduce库首先会将输入的文件拆分为M份，每份大小通常为16MB至64MB（具体的大小可以由用户通过可选参数来控制）。
然后便在集群中的一组机器上启动多个程序的副本。

其中一个程序的副本是特殊的-即master。剩下的程序副本都是worker, worker由master来分配任务。
这里有M个map任务和R个reduce任务需要分配。master选择空闲的worker，并且为每一个被选中的worker分配一个map任务或一个reduce任务。

一个被分配了map任务的worker，读取被拆分后的对应输入内容。
从输入的数据中解析出key/value键值对，并将每一个kv对作为参数传递给用户自定义的map函数。
map函数产生的中间态key/value键值对会被缓存在内存之中。
缓存在内存中的kv对会被周期性地写入通过分区函数所划分出的R个磁盘区域内。
这些在本地磁盘上被缓冲的kv对的位置将会被回传给master，master负责将这些位置信息转发给后续执行reduce任务的worker。
当一个负责reduce任务的worker被master通知了这些位置信息(map任务生成的中间态kv对数据所在的磁盘信息)，
该worker通过远过程调用(RPC)从负责map任务的worker机器的本地磁盘中读取被缓存的数据。
当一个负责reduce任务的worker已经读取了所有的中间态数据，将根据中间态kv对的key值进行排序，因此所有拥有相同key值的kv对将会被分组在一起。
需要排序的原因是因为通常很多不同的key(的kv对集合)会被映射到同一个reduce任务中去。如果(需要排序的)中间态的数据量过大，无法完全装进内存时，将会使用外排序。

负责reduce任务的worker迭代所有被排好序的中间态数据，并将所遇到的每一个唯一的key值和其对应的中间态value值集合传递给用户自定义的reduce函数。
reduce函数所产生的输出将会追加在一个该reduce分区内的、最终的输出文件内。

当所有的map任务和reduce任务都完成后，master将唤醒用户程序。此时，调用MapReduce的用户程序(的执行流)将会返回到用户代码中。

在成功的完成计算后，MapReduce执行的输出结果将被存放在R个输出文件中(每一个reduce任务都对应一个输出文件，输出文件的名字由用户指定)。
通常，用户无需将这R个输出文件合并为一个文件 - 他们通常传递这些文件，将其作为另一个MapReduce调用的输入，或者由另一个能处理多个被分割的输入文件的分布式应用所使用。

## Master数据结构
master中维护了一些数据结构。对于每一个map和reduce任务，master存储了对应的任务状态(闲置的，运行中，或者已完成)，以及worker机器的id(针对非空闲的任务)。
master是一个管道，将中间态文件的位置信息从map任务传递给reduce任务。
因此，对于每个已完成的map任务，master存储了由map任务生成的R个中间态文件区域的位置和大小。
当map任务完成时，master将更新接受到的(中间态文件区域)位置和大小信息。
这些信息的变更会以增量的方式推送给运行中的reduce任务。

## 容错
由于MapReduce库是被设计用于在几百或几千台机器上进行大规模数据处理的，所以该库必须能优雅地处理机器故障。

master会周期性的ping每一个worker。
如果在一定的时间内没有接收到来自某一worker的响应，master将会将worker标记为有故障(failed)。
所有由该worker完成的map任务将会被重置回初始状态，因此这些map任务能被其它worker去调度执行。
类似的，任何在这个有故障的worker上处理中的map或reduce任务状态也将被重置为初始化，并且(这些被重置的任务)能够被重新调度执行。

已完成的map任务在故障时需要被重复执行的原因在于map任务的输出是被存储在故障机器的本地磁盘上的，因此无法被访问到(宕机或者网络不通等情况)。
而已完成的reduce任务不需要重复执行的原因在于其输出是被存储在全局的文件系统中的。

当一个map任务在worker A上被首次执行，不久后又被worker B执行(因为worker A发生了故障)，所有执行reduce任务的worker将会被通知需要重新执行。
所有还没有从worker A处读取(完整)数据的reduce任务将改为从worker B处读取数据。

MapReduce能从大范围的worker故障中迅速的恢复。
例如，在一个MapReduce操作运行期间内，一个正在运行的集群上的一次网络维护导致了80台机器在几分钟内无法访问的。
MapReduce的master只需要将这些无法访问的机器上的任务重新的执行，然后继续向前推进，最终完成这个MapReduce操作。

## Master故障
可以简单的让master周期性的将上述的master数据结构以检查点的形式持久化。
如果master任务机器宕机了，一个新的master备份机器将会从最新的检查点状态处启动。
然而，考虑到只有一台master机器，是不太可能出现故障的；因此如果master故障了，我们当前的实现会中止MapReduce计算。
客户端可以检查master的这些状态，并根据需要重试MapReduce操作。

Semantics in the Presence of Failures(面对故障时的语义)
当用户提供的map和reduce算子都是基于其输入的确定性函数时，我们所实现的分布式(计算)的输出与整个程序的一个无故障的顺序串行执行后会的输出(结果)是一样的。

我们依赖map和reduce任务输出结果的原子性提交机制来实现这一特性。
每一个处理中的任务将它们的输出写入其(任务)私有的临时文件中。
一个reduce任务产生一个这样的文件，同时一个map任务产生R个这样的文件(共R个文件，R个reduce任务每个各对应一个文件)。
当一个map任务完成后，对应worker会发送给master一个消息，消息内包含了这R个临时文件名字的。
如果master接受到一个(已被标记为)已完成状态任务的完成消息时，其会忽略该消息。
否则，将这R个文件的名字记录到master(维护)的数据结构中。

当一个reduce任务完成了，执行reduce任务的worker会原子性的将临时的输出文件重命名为最终的输出文件。
如果在多台机器上有相同的reduce任务被执行，在同一个最终输出文件上将会被执行多次重命名调用。
我们依赖底层文件系统所提供的原子性重命名操作来保证最终文件系统中恰好只保存了一次reduce任务执行的数据。

我们绝大多数的map和reduce算子都是确定性的(即：输出完全由输入决定，同样地输入一定有着同样地输出)，
在这种情况下我们(分布式架构下并行执行)的语义等价于(单机单线程)顺序串行执行，这一事实使得程序员很容易理解他们程序的行为。
当map或reduce算子是非确定性的，我们提供了一个稍弱但依然合理的语义。
存在非确定性算子的情况下，一个特定reduce任务R1的输出等同于R1在非确定性程序下(单机单线程)顺序串行执行的输出。
然而，另一个与R1不同的reduce任务R2的输出将会对应于R2在一个不同的非确定程序中以顺序串行执行的输出。

考虑下目前有一个map任务M和两个reduce任务R1和R2。
假设e(Ri)代表Ri任务已经被提交的一次执行(恰好只执行一次)。
由于e(R1)可能在一次执行中读取M任务产生的输出，同时e(R2)可能会在另一次执行中读取M任务产生的输出，此时将会出现弱语义。

## 局限性
在我们的计算环境中，网络带宽是一个相对稀缺的资源。
我们利用输入的数据(被GFS管理)被存储在组成我们集群的机器的本地磁盘上这一事实来节约网络带宽。
GFS将每个文件分割为64MB的块，同时为每一个块存储几个备份(通常是3个副本)在不同的机器上。
MapReduce的master调度map任务时将输入文件的位置信息考虑在内，尽量在包含对应输入数据副本的机器上调度执行一个map任务。
如果任务失败了，调度map任务时会让执行任务的机器尽量靠近任务所需输入数据所在的机器(举个例子，被选中的worker机器与包含数据的机器位于同一网络交换机下)。
当集群中的相当一部分worker都在执行大型MapReduce操作时，绝大多数的输入数据都在本地读取从而不会消耗网络带宽。

## 任务粒度
如上所述，我们将map阶段的任务拆分为M份，同时将reduce阶段的任务拆分为R份。
理想情况下，M和R的值都应该远大于worker机器的数量。
让每一个worker执行很多不同的任务可以提高动态负载均衡的效率，
同时也能加快当一个worker故障时的恢复速度：（故障worker机器上）很多已经完成的map任务可以分散到所有其它的worker机器上去(重新执行)。

在我们的实现中，对M和R的实际大小做了限制，因为master必须O(M+R)的调度决定，同时要保持O(MR)个如上所处的内存状态。
(然而这对于内存的总体使用率来说影响还是较小的：这O(MR)份的状态里，构成每个map/reduce任务对的数据(只)占大约1字节。)

除此之外，用户通常会限制R的大小，因为每一个reduce任务的输出最后都会在一个被拆分的输出文件中。
实际上，我们倾向于设置M的大小使得每个独立任务所需的输入数据大约在16MB至64MB之间(使得上文所述的局部性优化效果最好), 同时我们设置R的大小为我们预期使用worker机器数量的小几倍。
我们执行MapReduce计算时，通常使用2000台worker机器，并设置M的值为200000，R的值为5000。

## 后备任务
导致MapReduce运算总耗时变长的一个常见的原因是存在“落伍者”：即一台机器花费了异常长的时间去完成计算中最后的几个map或reduce任务。
导致“落伍者”出现的原因多种多样。
举个例子，一台有着坏磁盘的机器可能会在(读取磁盘时)频繁进行纠错，使得磁盘的读取性能从每秒30MB下降到每秒1MB。
集群调度系统可能还将其它任务也调度到了这台机器上，由于CPU、内存、本地磁盘或网络带宽的竞争，使得MapReduce代码的执行变得更加的缓慢。
我们最近遇到的一个问题是由机器初始化代码中的一个bug导致的，其禁用了处理器的缓存：受到影响的机器其计算速度(比正常情况下)慢了100倍以上。

我们有一个通用的机制来减轻“落伍者”问题带来的影响。
当一个MapReduce运算接近完成时，master将会调度剩下的处理中的任务进行后备执行(backup executions)。
无论是主执行完成还是后备执行完成，这些任务都会被标记为已完成。
我们已对这个机制进行了优化，使得这一操作令所使用的计算资源增加通常不会超过几个百分点。
我们发现这一操作明显减少了大型MapReduce操作的完成时间。
例如，如果禁用后备任务这一机制，在5.3节中所述的排序程序将多花费44%的时间才能完成。

