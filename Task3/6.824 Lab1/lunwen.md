
4 Refinements(改进)
尽管已提供的编写简单Map和Reduce函数的功能能满足大多数需求，但我们还发现了一些有价值的拓展。
本章节将对此进行介绍。

4.1 Partitioning Function(分区函数)
MapReduce用户期望能指定reduce任务/输出文件的数量。
在这些任务中，使用一个基于中间态key的分区函数对数据进行分区。
(我们)提供了一个使用哈希取模的默认分区函数(例如：“hash(key) mod R”)。
这往往会得到一个非常均衡的分区结果。
然而在有些情况下，使用其它的一些基于key的分区函数对数据进行分区是很有用的。
举个例子，有时(map任务)输出的key是URL，且我们希望同一个主机上的所有条目最后都写入同一个输出文件中。
为了支持这种场景，MapReduce库的用户可以提供一个自定义的分区函数。
举个例子，使用“hash(Hostname(urlkey)) mod R”作为分区函数，就可以使得来自同一个主机的所有URL(条目)最终都写入同一个输出文件中。

4.2 Ordering Guarantees(有序性保证)
我们保证在给定的分区内，中间态的k/v对是以中间态key值递增的顺序处理的。
这一有序性保证使得能简单的为每个分区生成一个已排序的输出文件，
当输出文件的格式需要支持基于key来进行高效随机查找时(这一机制)会很有价值,或者用户需要已经排好序的数据时会很方便。

4.3 Combiner Function(组合器函数)
在一些情况下，每个map任务所生成的中间态key存在明显的重复，同时用户自定义的reduce函数具备可交换性和可结合性。
2.1章节中的单词计数的示例程序就是一个很好的例子。
由于单词出现的频率遵循齐夫分布(Zipf distribution)，因此每一个map任务都将生成几百或几千的形如<the,1>的记录。
所有的这些计数将通过网络发送给一个单独的reduce任务，然后再通过reduce函数累加它们而生成一个数字。
我们允许用户指定一个可选的Combiner函数,在数据通过网络发送前该函数将对数据进行不完全的合并。

Combiner函数能在每一个执行map任务的机器上执行。
通常情况下，combiner函数和reduce函数的代码实现是相同的。
reduce函数和combiner函数间唯一的不同在于MapReduce是如何处理函数的输出。
一个reduce函数的输出会写入最终的输出文件中。
而一个combiner函数的输出会被写入到一个中间态的文件中，并且将会发送给reduce任务。

部分合并可以明显加快某些MapReduce操作的速度。
附录A中包含了一个使用combiner的例子。

4.4 Input and Output Types(输入和输出的类型)
MapReduce库为多种不同格式输入数据的读取提供了支持。
例如，"文本"模式下将每一行的输入视为一个kv键值对：key是该行在文件中的偏移量，而value是该行的内容。
另一种所支持的常用格式则存储基于key排序的一连串kv键值对。
每一个输入类型的实现知道如何将输入的数据划分为有意义的区间，用以在一个独立的map任务中处理。
(举个例子，文本模式划分区间时确保了只会在每一行的边界上出现区间的划分)
通过提供一个简单的reader接口实现，用户可以增加支持一种新的输入类型，即使大多数用户只会使用一个或少数几个预定义的输入类型。

reader不一定只能通过读取文件来提供数据。
举个例子，很容易定义一个reader去数据库中读取记录，或者从被映射在内存中的数据结构中读取数据。

类似的，我们也支持多种不同格式的输出数据，且用户代码中可以轻松地支持新增的一种新输出类型。

4.5 Side-effects(副作用)
在某些场景下，MapReduce的用户发现从他们的map或reduce操作中生成辅助文件作为额外的输出可以为其带来一些便利。
我们依赖应用程序的作者(自己在程序中保证)使得这些副作用具有原子性和幂等性。
通常，应用程序会(将额外的输出)写入一个临时文件，并且一旦完全生成该文件后便原子性的重命名这一文件。

我们没有为单个任务生成多个文件的场景提供原子性二阶段提交的支持。
因此，会生成多个输出文件且具有跨文件一致性需求的任务应该是确定性的（任务是确定性函数算子）。
在我们的实践中，这一限制并没有带来什么问题。

4.6 Skipping Bad Records(跳过错误的记录)
有时用户的代码中存在一些bug，造成了Map或Reduce函数在处理某些数据时一定会崩溃。这些bug会阻止MapReduce操作的完成。
通常的做法是修复这个bug，但有时这是行不通的；可能这个bug位于三方库中，且无法获得其源代码。
当然，有时忽略掉少量的数据是可接受的，比如对一个大型数据集上进行统计分析时。
我们提供了一个可选的执行模式，当MapReduce库检测到某些记录一定会导致崩溃时，跳过这些记录并继续向前推进。

每个worker进程都安装了一个信号处理器，用于捕获段异常(segmentation violations)和总线错误(bus errors)。
在调用用户的Map或Reduce操作前，MapReduce库会将参数的序列号存储在一个全局变量中。
如果用户代码产生了一个信号，则信号处理器将会向MapReduce的master发送一个包含了(该参数)序列号的"最后喘息(last gasp)"UDP包。
当master一个特定的记录不止一次的导致故障时，master会指示对应的Map或Reduce任务在下一次重新执行时应该跳过该记录。

4.7 Local Execution(本地执行)
在实际计算发生在分布式系统中时，调试Map或Reduce函数会变得很棘手，通常由master动态的在几千台机器上决定工作的分配。
为了更利于调试、分析和小规模的测试，我们开发了一个(运行在本地机器上的)MapReduce库的可替代实现，该库能让所有的MapReduce工作在本地机器上顺序执行。
控制权被交给了用户,使得计算可以被限制在指定的Map任务中。
用户通过一个特殊的标志来调用他们的程序，然后可以轻松地使用任何他们觉得好用的调试或者测试工具(例如：gdb)。

4.8 Status Information(状态信息)
master机器运行了一个内置地Http服务器，并提供了一系列地状态信息页面供用户访问。
状态信息页面会展示计算的进度，例如有多少任务已经完成，多少任务正在执行中，输入数据的字节数，中间数据的字节数，输出数据的字节数，处理速度等等信息。
页面也包含了指向每个任务对应的标准误差(standard error)和标准输出文件的链接。
用户可以使用这些数据预测还要多长时间完成计算，以及是否需要为该计算投入更多资源。
这些页面也可用于找出为什么实际的计算比所预期的要慢的原因。

此外，高级状态页面展示了哪些worker机器发生了故障，以及哪些map和reduce任务在执行时发生了故障。
在尝试调试用户代码中的bug时这些信息会很有用。

4.9 Counters(计数器)
MapReduce库提供了一个计数器的功能，用于计数不同事件出现的次数。
例如，用户代码可能会想要统计已经处理过的单词总数或者被编入德文文档的索引数等等。

为了使用这一功能，用户代码中需要创建一个名为计数器的对象，然后在Map或Reduce函数中以恰当的方式对计数器进行累加操作。
例如：
Count* uppercase;
uppercase = GetCounter("uppercase");
map(String name, String contents) :
    for each word w in contents:
        if(isCapitalized(w)):
            uppercase->Increment();
        EmitIntermediate(w,"1");

独立worker机器中的counter值会周期性的传递给master(在ping响应包中附带)
master将来自已经成功完成的map和reduce任务中的counter值聚合在一起，并在MapReduce任务完成时返回给用户代码。
当前的counter值也会展示在master的状态页上，使得用户可以看到实时的计算进度。
在聚合counter值时，master消除了同一个map或reduce任务多次执行的影响，避免了重复计数。
(多次执行出现的原因是我们的备份任务或任务故障时的重复执行导致的)

有些counter值是由MapReduce自行维护的，例如已处理的输入k/v对的数量和已生成的输出k/v对的数量。

用户发现计数器功能能很好的用于检查MapReduce操作的行为是否正常。
例如，在某些MapReduce操作中，用户代码想要确保已生成的k/v对数量严格等于已处理的输入k/v对数量，或者确保已处理的德语文档数量在已处理的全部文档中的占比是否处于一个可接受的比例内。

5 Performance(性能)
在这一章节，我们通过在大型机器集群上运行的两个MapReduce计算来测量MapReduce的性能。
其中一个计算是在大约1TB的数据中检索特定的模式。
另一个计算是对大约1TB的数据进行排序。

上述两个程序代表了现实中大多数MapReduce用户所编写的程序，一类程序将数据从一种表示方式转化为另一种表示方式，而另一类程序则从一个大的数据集中提取出少量感兴趣的数据。

5.1 Cluster Configuration(集群配置)
所有的程序都在一个由大约1800台机器组成的集群上被执行。
每台机器都配置有两颗开启了超线程功能的、2GHZ主频的Intel至强处理器，4GB的内存，两块160GB容量的IDE硬盘，以及一条千兆的以太网链路。
所有机器都被安置在一个双层的树形交换网络中，根节点处的总可用网络带宽大概为100-200GB每秒。
所有的机器都位于同一个主机托管设施(hosting facility)内，因此任意一对机器间的(网络交互的)往返时间都低于1毫秒。

在4GB的内存中，大约1-1.5GB的内存是为集群上要运行的其它任务而保留的。
任务是在周末的下午执行的，(因为)这个时间点CPU、硬盘和网络一般都是空闲的。

5.2 Grep(Globally search a Regular Expression and Print 基于正则表达式的全局搜索并打印)
grep程序扫描通过扫描10^10个100字节大小的记录，搜索一个相对比较少见的3字符模式(这个模式只出现在92337条记录中)。
输入数据被分割为大约64MB大小的块(M = 15000)，并且完整的输出被放在了一个文件中(R = 1)。

图二展示了随时间推移的计算进度。
Y轴标识着扫描输入数据的速率。
随着越来越多的机器被分配给当前MapReduce计算，扫描输入数据的速率也越来越快，并且当分配了1764个worker机器时其峰值达到了30GB每秒。
当map任务完成后，扫描输入数据的速率开始下降并在计算执行到大约80秒的时候降至0。
整个计算从开始到结束大概耗时150秒。
这其中包括了一分钟左右的启动开销。
这一开销是由于需要将程序分发到所有的worker机器上，以及为了打开1000个输入文件集合而与GFS交互并获得局部性优化信息的延迟。

5.3 Sort(排序)
这个排序程序对10^10个100字节大小的记录进行排序(大约1TB的数据)。
这个程序是参照TeraSort基准测试程序而编写的。

排序程序包含了少于50行的用户代码。
一个三行的Map函数从一个文本行中提取出一个10字节大小的、用于排序的key并且发出该key，并将原始的文本行作为value而生成中间态的k/v键值对。
我们使用内置的恒等函数(Identity function)作为Reduce算子。
这个函数传入中间态的k/v键值对，并且不做任何修改的将之作为输出的k/v键值对。
最终完成排序的输出被写入了一个双向复制的GFS文件集合中(即程序总共写入、输出了2TB的数据)。

如上所述，输入的数据被分给为64MB的块(M = 15000)。
我们将排好序后的输出数据分割为4000个文件(R = 4000)。
分区函数基于key的初始字节值将其分割为R份。

我们的基准测试中内置的分区函数是了解key值具体分布的。
在一个常规的排序程序中，我们会预先插入一个MapReduce操作，该操作将会收集key值的一个样本并且基于key值样本的分布情况来计算最终排序时需要的分割点。

图3的a部分展示了一个排序程序的正常执行过程。
左上角的图表标识了输入数据读取的速率。
输入数据速率的峰值为13GB每秒，由于所有的map任务都在200秒内完成了因此其非常快速地降到了零。
请注意输入速率是小于上述地grep程序的。
这是因为排序的map任务有一半的耗时和I/O带宽用于将的中间态的输出写入它们机器的本地磁盘。
而相应的，grep任务的中间态输出则可以忽略不计。

左边排中间的图表标识了map任务通过网络将数据发送给reduce任务的速率。
这一转换在第一个map任务完成不久后便开始了。
图表中的第一个高峰对应着第一批的大约1700个reduce任务(整个MapReduce分配了1700台机器，并且每一台机器同一时间至多只能执行一个reduce任务)
大概执行了300秒的计算时，第一批的一些reduce任务陆续完成并且剩余的reduce任务继续转换数据。
所有的转换大概在计算执行了600秒时完成。

左下方的图表标识了reduce任务将排序好的数据写入最终的输出文件的速率。
在第一个转换(shuffling)阶段结束到开始写入之间存在一点延迟，其原因是机器此时正忙于对中间态的数据进行排序。
写入数据的以2-4GB每秒的速率持续了一段时间。所有的写入大约在计算执行至850秒左右时完成。
包括启动的开销在内，整个计算过程共耗时891秒。
这与TeraSort基准测试目前已报告的最快记录很相近。

有几点值得注意：

输入的速录比转换和输出的速率要高很多，其原因在于我们进行了局部性优化。大多数的数据是从本地的硬盘中读取的，从而避免使用我们相对有限的网络带宽。
转换速率比输出速率要高很多，其原因在于输出阶段写入了已排序数据的两个备份(出于可靠性和可用性的考虑，我们构建了两个输出数据的备份)。
我们写入两个备份的原因在于这是我们底层文件系统所提供的可靠性和可用性的机制。
如果底层文件系统使用纠错码(Erasure Coding)来代替复制(来保证可靠性)，则需要写入数据时所需要的网络带宽将减少很多。

5.4 Effect of Backup Tasks(后备任务的影响)
在图3的b部分，我们展示了禁用后备任务时排序程序的执行状况。
执行流与图3的a部分很相似，除了最后面有一个非常长的尾部，且其几乎没有任何写入发生(注意观察代表done的那条竖线)。
在960s后，除了5个reduce任务外其它任务都已经完成。
然而最后几个“落伍者”任务直到300秒后才相继完成。整个计算过程共花费了1283秒，(相比正常执行的情况)增加了44%的耗时。

5.5 Machine Failures(机器故障)
在图3的c部分，我们展示了一个排序程序的执行流程，在其计算过程中我们故意在几分钟内杀死(killed)了1746台worker机器中的200台机器(的worker进程)。
底层的集群调度器立即在这些机器上重新启动新的worker进程(因为只是杀掉了worker进程，机器依然是正常工作的)。

worker进程被杀死时展示一个负的输入速率，因为之前已完成的任务失效了(因为对应的map worker被杀掉了)并且这些任务需要被重新执行。
map任务的重新执行相对来说是比较快的。
包括启动开销在内，整个计算过程共耗时933秒（相较于正常执行时的耗时，只增加了5%）

6 Experience(经验)
我们于2003年2月编写了第一版的MapReduce库，并且在2003年的8月对其进行了重大改进，其中包括局部性优化、跨worker机器间任务执行的动态负载均衡等等。
从那时起，我们惊喜的看到MapReduce库被广泛的应用于我们工作中所遇到的各种问题上。
MapReduce库已在谷歌内的许多领域中被广泛的使用，其中包括：

大规模的机器学习问题
Google新闻和Froogle产品的聚类问题(clustering problems)
基于常见查询所产出的报告提取数据(例如，Google Zeitgeist (注：Google开发的一款网络查询分析程序))
基于新实验和产品的网页提取相关属性(例如，从用于本地化搜索的大型网页语料库中提取地理位置)
大规模的图计算

图4展示了登记在我们主要的源码管理系统中的独立MapReduce程序数量随着时间的推移有着显著的增长，
从2003年年初的0个,再到2004年的9月有了接近900个独立的MapReduce程序实例了。
MapReduce如此成功的原因在于其使得编写一个简单的程序，并在半小时内于上千台机器上高效的运行成为了可能，这极大地加快了开发和原型设计的周期。
此外，MapReduce允许没有任何分布式或并行系统开发经验的程序员得以轻松的利用大量的(计算)资源。

在每个job完成时，MapReduce库会以日志的形式记录对应job所使用的计算资源的统计信息。
在表1中，我们展示了谷歌在2004年8月所运行的MapReduce job的一个子集的(所使用计算资源的)一些统计信息。